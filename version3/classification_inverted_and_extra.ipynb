{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf6a1420",
      "metadata": {
        "id": "cf6a1420"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "26cfa340",
      "metadata": {
        "id": "26cfa340"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn,optim\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effc4c9e",
      "metadata": {
        "id": "effc4c9e"
      },
      "source": [
        "## Download and Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "39b2cf22",
      "metadata": {
        "id": "39b2cf22"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.MNIST('/content',train=True,download=True)\n",
        "test_set = datasets.MNIST('/content',train=False,download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 10\n",
        "num_classes = 10"
      ],
      "metadata": {
        "id": "yWzsKWJB-qSJ"
      },
      "id": "yWzsKWJB-qSJ",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e42069f6",
      "metadata": {
        "id": "e42069f6"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8ce40f00",
      "metadata": {
        "id": "8ce40f00"
      },
      "outputs": [],
      "source": [
        "# inversion and normalization\n",
        "invert = lambda image : 1 - image # function to invert the image\n",
        "normalize = lambda image : image / 255 # function for bringing pixel values in range [0,1]\n",
        "\n",
        "def preprocessing(dataset):\n",
        "    dataset_images = dataset.data.numpy() # convert the dataset into numpy array\n",
        "    dataset_labels = dataset.targets.numpy() # convert the labels into numpy array\n",
        "    dataset_images = normalize(dataset_images)\n",
        "    dataset_images = invert(dataset_images)\n",
        "    return dataset_images,dataset_labels\n",
        "\n",
        "train_images,train_labels = preprocessing(train_set)\n",
        "test_images,test_labels = preprocessing(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b53b31",
      "metadata": {
        "id": "57b53b31"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "eb717edb",
      "metadata": {
        "id": "eb717edb"
      },
      "outputs": [],
      "source": [
        "def generate_labels(dataset_images,image_type):\n",
        "  labels = np.full(shape=(dataset_images.shape[0]),fill_value=[image_type])\n",
        "  return labels\n",
        "\n",
        "# function for adding salt\n",
        "def put_salt(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 3 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.4:\n",
        "          flattened_image[j] = 1\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for adding pepper\n",
        "def put_pepper(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 3 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    # negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.6:\n",
        "          flattened_image[j] = 0\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for inverting data\n",
        "def get_inverted_data(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 3 + 1]\n",
        "    original_images = invert(source_dataset_images[image_indices]) # get the original images and invert them\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function to prepare the dataset for a given digit\n",
        "def prepare_training_data(dataset_images,dataset_labels,class_num):\n",
        "    indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "    indices = indices.flatten()\n",
        "    # get the images for making positive dataset\n",
        "    dataset_images_positive = dataset_images[indices] # images consisting of positive class\n",
        "    dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "    # print(dataset_images_positive.shape)\n",
        "    # get the images for making negative dataset\n",
        "    dataset_images_negative = np.empty((0,28,28),dtype=np.float32)\n",
        "    dataset_images_negative = put_salt(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    dataset_images_negative = put_pepper(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    dataset_images_negative = get_inverted_data(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    # print(dataset_images_negative.shape)\n",
        "    dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "    # concatenate the negative and positive datasets\n",
        "    modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "    modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "    tensor_x = torch.Tensor(modified_dataset_images)\n",
        "    tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "    new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "    return new_dataset\n",
        "\n",
        "def prepare_testing_data(dataset_images,dataset_labels,class_num):\n",
        "  positive_indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "  positive_indices = positive_indices.flatten()\n",
        "  # get the images for making positive dataset\n",
        "  dataset_images_positive = dataset_images[positive_indices] # images consisting of positive class\n",
        "  dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "  # print(dataset_images_positive.shape)\n",
        "  # get the images for making negative classes for testing\n",
        "  negative_indices = np.asarray(np.where(dataset_labels != class_num))\n",
        "  negative_indices = negative_indices.flatten()\n",
        "  # get the images for making negative dataset\n",
        "  dataset_images_negative = dataset_images[negative_indices]\n",
        "  dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "  # print(dataset_images_negative.shape)\n",
        "  # concatenate the negative and positive datasets\n",
        "  modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "  modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "  tensor_x = torch.Tensor(modified_dataset_images)\n",
        "  tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "  new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ef3a88",
      "metadata": {
        "id": "78ef3a88"
      },
      "source": [
        "## Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.fc3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i8ZNAjNZ-Iri"
      },
      "id": "i8ZNAjNZ-Iri",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "tSL2BWDd-MSY"
      },
      "id": "tSL2BWDd-MSY"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "5433be70",
      "metadata": {
        "id": "5433be70"
      },
      "outputs": [],
      "source": [
        "def train(model,trainloader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
        "  # train the model\n",
        "  model.train()\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch, (images,scores) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      # compute prediction error\n",
        "      output = model(images)\n",
        "      loss = criterion(output,scores)\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    # else:\n",
        "    #   print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "BpT0936aAfTz"
      },
      "id": "BpT0936aAfTz"
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,testloader,classes):\n",
        "  correct = np.zeros(classes,dtype=np.float64)\n",
        "  total = np.zeros(classes,dtype=np.float64)\n",
        "  with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "      output = model(images)\n",
        "      pred = torch.argmax(output,dim=1)\n",
        "      for i in range(len(labels)):\n",
        "        correct[labels[i]] += (pred[i] == labels[i])\n",
        "        total[labels[i]] += 1\n",
        "  print(f\"Accuracy : {np.sum(correct) / np.sum(total)}\")\n",
        "  return np.divide(correct,total)"
      ],
      "metadata": {
        "id": "Yf0lJ-gSAeb5"
      },
      "id": "Yf0lJ-gSAeb5",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the training set"
      ],
      "metadata": {
        "id": "pNKh_D3E-dXM"
      },
      "id": "pNKh_D3E-dXM"
    },
    {
      "cell_type": "code",
      "source": [
        "for class_num in range(num_classes):\n",
        "  train_class_set = prepare_training_data(train_images,train_labels,class_num) # fetch the training set for a class\n",
        "  test_class_set = prepare_testing_data(test_images,test_labels,class_num)\n",
        "  train_loader = DataLoader(train_class_set,batch_size=batch_size,shuffle=True)\n",
        "  test_loader = DataLoader(test_class_set,batch_size=batch_size,shuffle=True) # take original testing set of all classes\n",
        "  model = NeuralNetwork()\n",
        "  model = train(model,train_loader)\n",
        "  print(test(model,test_loader,2))"
      ],
      "metadata": {
        "id": "jKMZUGyU-k9F",
        "outputId": "8195e2b9-f7e9-4104-d584-9dac0eb3f824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jKMZUGyU-k9F",
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.6494\n",
            "[0.96326531 0.61529933]\n",
            "Accuracy : 0.6484\n",
            "[0.97268722 0.60688099]\n",
            "Accuracy : 0.294\n",
            "[0.93023256 0.22078501]\n",
            "Accuracy : 0.2163\n",
            "[0.99009901 0.12936596]\n",
            "Accuracy : 0.4844\n",
            "[0.95010183 0.43368818]\n",
            "Accuracy : 0.1151\n",
            "[0.99103139 0.02931489]\n",
            "Accuracy : 0.4922\n",
            "[0.96033403 0.44260119]\n",
            "Accuracy : 0.3252\n",
            "[0.94163424 0.25456977]\n",
            "Accuracy : 0.4553\n",
            "[0.96919918 0.39984489]\n",
            "Accuracy : 0.337\n",
            "[0.98017839 0.26482038]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MNIST classification.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}