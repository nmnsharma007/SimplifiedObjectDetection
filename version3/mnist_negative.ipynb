{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf6a1420",
      "metadata": {
        "id": "cf6a1420"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "26cfa340",
      "metadata": {
        "id": "26cfa340"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn,optim\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effc4c9e",
      "metadata": {
        "id": "effc4c9e"
      },
      "source": [
        "## Download and Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "39b2cf22",
      "metadata": {
        "id": "39b2cf22"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.MNIST('/content',train=True,download=True)\n",
        "test_set = datasets.MNIST('/content',train=False,download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 5\n",
        "num_classes = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "yWzsKWJB-qSJ"
      },
      "id": "yWzsKWJB-qSJ",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e42069f6",
      "metadata": {
        "id": "e42069f6"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "8ce40f00",
      "metadata": {
        "id": "8ce40f00"
      },
      "outputs": [],
      "source": [
        "# inversion and normalization\n",
        "invert = lambda image : 1 - image # function to invert the image\n",
        "normalize = lambda image : image / 255 # function for bringing pixel values in range [0,1]\n",
        "\n",
        "def preprocessing(dataset):\n",
        "    dataset_images = dataset.data.numpy() # convert the dataset into numpy array\n",
        "    dataset_labels = dataset.targets.numpy() # convert the labels into numpy array\n",
        "    dataset_images = normalize(dataset_images)\n",
        "    dataset_images = invert(dataset_images)\n",
        "    return dataset_images,dataset_labels\n",
        "\n",
        "train_images,train_labels = preprocessing(train_set)\n",
        "test_images,test_labels = preprocessing(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b53b31",
      "metadata": {
        "id": "57b53b31"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "eb717edb",
      "metadata": {
        "id": "eb717edb"
      },
      "outputs": [],
      "source": [
        "def generate_labels(dataset_images,image_type):\n",
        "  labels = np.full(shape=(dataset_images.shape[0]),fill_value=[image_type])\n",
        "  return labels\n",
        "\n",
        "# function for adding salt\n",
        "def put_salt(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.4:\n",
        "          flattened_image[j] = 1\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    for i in range(len(negative_images)):\n",
        "      flattened_image = negative_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.2:\n",
        "          flattened_image[j] = 1\n",
        "      negative_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,negative_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for adding pepper\n",
        "def put_pepper(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.6:\n",
        "          flattened_image[j] = 0\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    for i in range(len(negative_images)):\n",
        "      flattened_image = negative_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.8:\n",
        "          flattened_image[j] = 0\n",
        "      negative_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,negative_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for inverting data\n",
        "def get_inverted_data(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    np.random.shuffle(image_indices)\n",
        "    image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = invert(source_dataset_images[image_indices]) # get the original images and invert them\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    return target_dataset\n",
        "\n",
        "# add some data from already available negative data\n",
        "def add_negative_data(source_dataset_images,source_dataset_labels,target_dataset,class_num,amount):\n",
        "  image_indices = np.asarray(np.where(source_dataset_labels != class_num))\n",
        "  image_indices = image_indices.flatten()\n",
        "  np.random.shuffle(image_indices)\n",
        "  image_indices = image_indices[:amount] # take very less amount of negative data\n",
        "  original_images = source_dataset_images[image_indices]\n",
        "  target_dataset = np.concatenate((target_dataset,original_images))\n",
        "  return target_dataset\n",
        "\n",
        "# function to prepare the dataset for a given digit\n",
        "def prepare_training_data(dataset_images,dataset_labels,class_num,amount):\n",
        "    indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "    indices = indices.flatten()\n",
        "    # get the images for making positive dataset\n",
        "    dataset_images_positive = dataset_images[indices] # images consisting of positive class\n",
        "    dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "    # print(f\"Positive train data shape: {dataset_images_positive.shape}\")\n",
        "    # get the images for making negative dataset\n",
        "    dataset_images_negative = np.empty((0,28,28),dtype=np.float32)\n",
        "    dataset_images_negative = put_salt(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    dataset_images_negative = put_pepper(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    dataset_images_negative = get_inverted_data(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    dataset_images_negative = add_negative_data(dataset_images,dataset_labels,dataset_images_negative,class_num,amount)\n",
        "    # print(f\"Negative train data shape: {dataset_images_negative.shape}\")\n",
        "    dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "    # concatenate the negative and positive datasets\n",
        "    modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "    modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "    tensor_x = torch.Tensor(modified_dataset_images)\n",
        "    tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "    new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "    return new_dataset\n",
        "\n",
        "def prepare_testing_data(dataset_images,dataset_labels,class_num):\n",
        "  positive_indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "  positive_indices = positive_indices.flatten()\n",
        "  # get the images for making positive dataset\n",
        "  dataset_images_positive = dataset_images[positive_indices] # images consisting of positive class\n",
        "  dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "  # print(f\"Positive test data shape: {dataset_images_positive.shape}\")\n",
        "  # get the images for making negative classes for testing\n",
        "  negative_indices = np.asarray(np.where(dataset_labels != class_num))\n",
        "  negative_indices = negative_indices.flatten()\n",
        "  # get the images for making negative dataset\n",
        "  dataset_images_negative = dataset_images[negative_indices]\n",
        "  dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "  # print(f\"Negative test data shape: {dataset_images_negative.shape}\")\n",
        "  # concatenate the negative and positive datasets\n",
        "  modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "  modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "  tensor_x = torch.Tensor(modified_dataset_images)\n",
        "  tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "  new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78ef3a88",
      "metadata": {
        "id": "78ef3a88"
      },
      "source": [
        "## Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self,classes,channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(channels,4,5)\n",
        "        self.conv2 = nn.Conv2d(4,6,5)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(6*4*4, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.fc3 = nn.Linear(10, classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i8ZNAjNZ-Iri"
      },
      "id": "i8ZNAjNZ-Iri",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "tSL2BWDd-MSY"
      },
      "id": "tSL2BWDd-MSY"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5433be70",
      "metadata": {
        "id": "5433be70"
      },
      "outputs": [],
      "source": [
        "def train(model,trainloader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
        "  model = model.to(device)\n",
        "  # train the model\n",
        "  model.train()\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch, (images,scores) in enumerate(train_loader):\n",
        "      (images,scores) = (images.to(device),scores.to(device))\n",
        "      optimizer.zero_grad()\n",
        "      # compute prediction error\n",
        "      output = model(images)\n",
        "      loss = criterion(output,scores)\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    # else:\n",
        "    #   print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "BpT0936aAfTz"
      },
      "id": "BpT0936aAfTz"
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,testloader,classes):\n",
        "  correct = np.zeros(classes,dtype=np.float64)\n",
        "  total = np.zeros(classes,dtype=np.float64)\n",
        "  with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "      (images,labels) = (images.to(device),labels.to(device))\n",
        "      output = model(images)\n",
        "      pred = torch.argmax(output,dim=1)\n",
        "      for i in range(len(labels)):\n",
        "        correct[labels[i]] += (pred[i] == labels[i])\n",
        "        total[labels[i]] += 1\n",
        "  # print(f\"Accuracy : {np.sum(correct) / np.sum(total)}\")\n",
        "  return (np.sum(correct) / np.sum(total),np.divide(correct,total))"
      ],
      "metadata": {
        "id": "Yf0lJ-gSAeb5"
      },
      "id": "Yf0lJ-gSAeb5",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the training set"
      ],
      "metadata": {
        "id": "pNKh_D3E-dXM"
      },
      "id": "pNKh_D3E-dXM"
    },
    {
      "cell_type": "code",
      "source": [
        "# for class_num in range(num_classes):\n",
        "class_num = 2\n",
        "x = np.linspace(0,2000,num=21) # intervals of amount of negative data to be taken while training\n",
        "accuracy = []\n",
        "# print(x)\n",
        "\n",
        "for amount in x:\n",
        "  train_class_set = prepare_training_data(train_images,train_labels,class_num,int(amount)) # fetch the training set for a class\n",
        "  test_class_set = prepare_testing_data(test_images,test_labels,class_num)\n",
        "  train_loader = DataLoader(train_class_set,batch_size=batch_size,shuffle=True)\n",
        "  test_loader = DataLoader(test_class_set,batch_size=batch_size,shuffle=True) # take original testing set of all classes\n",
        "  model = NeuralNetwork(num_classes,1)\n",
        "  model = train(model,train_loader)\n",
        "  results = test(model,test_loader,2)\n",
        "  accuracy.append(results[0])\n",
        "  print(f\"Accuracy for {amount} samples of negative data: {results[0]}\")"
      ],
      "metadata": {
        "id": "jKMZUGyU-k9F",
        "outputId": "1e2394d2-d2d3-43ee-f5f2-2972f6da0e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "id": "jKMZUGyU-k9F",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for 0.0 samples of negative data: 0.1426\n",
            "Accuracy for 100.0 samples of negative data: 0.1368\n",
            "Accuracy for 200.0 samples of negative data: 0.192\n",
            "Accuracy for 300.0 samples of negative data: 0.1221\n",
            "Accuracy for 400.0 samples of negative data: 0.6854\n",
            "Accuracy for 500.0 samples of negative data: 0.1032\n",
            "Accuracy for 600.0 samples of negative data: 0.8569\n",
            "Accuracy for 700.0 samples of negative data: 0.7163\n",
            "Accuracy for 800.0 samples of negative data: 0.5947\n",
            "Accuracy for 900.0 samples of negative data: 0.6405\n",
            "Accuracy for 1000.0 samples of negative data: 0.8968\n",
            "Accuracy for 1100.0 samples of negative data: 0.8064\n",
            "Accuracy for 1200.0 samples of negative data: 0.1583\n",
            "Accuracy for 1300.0 samples of negative data: 0.6578\n",
            "Accuracy for 1400.0 samples of negative data: 0.145\n",
            "Accuracy for 1500.0 samples of negative data: 0.6019\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5545ce62c475>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_class_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# take original testing set of all classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-b6a73b57e4f0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, trainloader)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0;31m# compute prediction error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the accuracy against amount of negative data used"
      ],
      "metadata": {
        "id": "-2HC2e92-nRG"
      },
      "id": "-2HC2e92-nRG"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x,accuracy,'-s')\n",
        "plt.xlabel(\"Amount\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Amount of negative data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b29WGaUJ-sCS"
      },
      "id": "b29WGaUJ-sCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the data"
      ],
      "metadata": {
        "id": "7S7_9D4BQJNf"
      },
      "id": "7S7_9D4BQJNf"
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_set = prepare_training_data(train_images,train_labels,class_num,10) # fetch the training set for a class\n",
        "test_class_set = prepare_testing_data(test_images,test_labels,class_num)\n",
        "train_loader = DataLoader(train_class_set,batch_size=64,shuffle=True)\n",
        "test_loader = DataLoader(test_class_set,batch_size=64,shuffle=True) # take original testing set of all classes\n",
        "dataiter = iter(train_loader)\n",
        "images,labels = dataiter.next()\n",
        "figure = plt.figure()\n",
        "print(labels)\n",
        "num_of_images = 60\n",
        "for index in range(1,num_of_images+1):\n",
        "    plt.subplot(6,10,index)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images[index].numpy().squeeze(),cmap='gray_r')"
      ],
      "metadata": {
        "id": "76KzvLzSQCFy"
      },
      "id": "76KzvLzSQCFy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MNIST classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}