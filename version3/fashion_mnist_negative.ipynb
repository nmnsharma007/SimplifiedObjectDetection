{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cf6a1420",
      "metadata": {
        "id": "cf6a1420"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "26cfa340",
      "metadata": {
        "id": "26cfa340"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from torchvision import datasets, transforms\n",
        "from torch import nn,optim\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score,accuracy_score,confusion_matrix,roc_auc_score,roc_curve\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch.nn.functional as F\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effc4c9e",
      "metadata": {
        "id": "effc4c9e"
      },
      "source": [
        "## Download and Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39b2cf22",
      "metadata": {
        "id": "39b2cf22"
      },
      "outputs": [],
      "source": [
        "train_set = datasets.FashionMNIST('/content',train=True,download=True)\n",
        "test_set = datasets.FashionMNIST('/content',train=False,download=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "epochs = 20\n",
        "num_classes = 10\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "yWzsKWJB-qSJ"
      },
      "id": "yWzsKWJB-qSJ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e42069f6",
      "metadata": {
        "id": "e42069f6"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8ce40f00",
      "metadata": {
        "id": "8ce40f00"
      },
      "outputs": [],
      "source": [
        "# inversion and normalization\n",
        "invert = lambda image : 1 - image # function to invert the image\n",
        "normalize = lambda image : image / 255 # function for bringing pixel values in range [0,1]\n",
        "\n",
        "def preprocessing(dataset):\n",
        "    dataset_images = dataset.data.numpy() # convert the dataset into numpy array\n",
        "    dataset_labels = dataset.targets.numpy() # convert the labels into numpy array\n",
        "    dataset_images = normalize(dataset_images)\n",
        "    # dataset_images = invert(dataset_images)\n",
        "    return dataset_images,dataset_labels\n",
        "\n",
        "train_images,train_labels = preprocessing(train_set)\n",
        "test_images,test_labels = preprocessing(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b53b31",
      "metadata": {
        "id": "57b53b31"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "eb717edb",
      "metadata": {
        "id": "eb717edb"
      },
      "outputs": [],
      "source": [
        "def generate_labels(dataset_images,image_type):\n",
        "  labels = np.full(shape=(dataset_images.shape[0]),fill_value=[image_type])\n",
        "  return labels\n",
        "\n",
        "# function for adding salt\n",
        "def put_salt(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    # np.random.shuffle(image_indices)\n",
        "    # image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.4:\n",
        "          flattened_image[j] = 1\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    for i in range(len(negative_images)):\n",
        "      flattened_image = negative_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.2:\n",
        "          flattened_image[j] = 1\n",
        "      negative_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,negative_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for adding pepper\n",
        "def put_pepper(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    # np.random.shuffle(image_indices)\n",
        "    # image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = source_dataset_images[image_indices] # get the original images\n",
        "    negative_images = invert(original_images)\n",
        "    for i in range(len(original_images)):\n",
        "      flattened_image = original_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.6:\n",
        "          flattened_image[j] = 0\n",
        "      original_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    for i in range(len(negative_images)):\n",
        "      flattened_image = negative_images[i].flatten()\n",
        "      for j in range(len(flattened_image)):\n",
        "        p = random.uniform(0,1)\n",
        "        if p <= 0.8:\n",
        "          flattened_image[j] = 0\n",
        "      negative_images[i] = np.reshape(flattened_image,(28,28))\n",
        "    target_dataset = np.concatenate((target_dataset,negative_images))\n",
        "    return target_dataset\n",
        "\n",
        "# function for inverting data\n",
        "def get_inverted_data(source_dataset_images,source_dataset_labels,target_dataset,class_num):\n",
        "    image_indices = np.asarray(np.where(source_dataset_labels == class_num))\n",
        "    image_indices = image_indices.flatten()\n",
        "    # np.random.shuffle(image_indices)\n",
        "    # image_indices = image_indices[:image_indices.shape[0] // 5 + 1]\n",
        "    original_images = invert(source_dataset_images[image_indices]) # get the original images and invert them\n",
        "    target_dataset = np.concatenate((target_dataset,original_images))\n",
        "    return target_dataset\n",
        "\n",
        "# add some data from already available negative data\n",
        "def add_negative_data(source_dataset_images,source_dataset_labels,target_dataset,class_num,amount):\n",
        "  image_indices = np.asarray(np.where(source_dataset_labels != class_num))\n",
        "  image_indices = image_indices.flatten()\n",
        "  np.random.shuffle(image_indices)\n",
        "  image_indices = image_indices[:amount] # take very less amount of negative data\n",
        "\n",
        "  original_images = source_dataset_images[image_indices]\n",
        "  target_dataset = np.concatenate((target_dataset,original_images))\n",
        "  inverted_images = invert(original_images)\n",
        "  target_dataset = np.concatenate((target_dataset,inverted_images))\n",
        "  salt_images = np.concatenate((np.copy(inverted_images),np.copy(original_images)))\n",
        "  pepper_images = np.concatenate((np.copy(inverted_images),np.copy(original_images)))\n",
        "  for i in range(len(salt_images)):\n",
        "    flattened_image = salt_images[i].flatten()\n",
        "    for j in range(len(flattened_image)):\n",
        "      p = random.uniform(0,1)\n",
        "      if p <= 0.6:\n",
        "        flattened_image[j] = 0\n",
        "    salt_images[i] = np.reshape(flattened_image,(28,28))\n",
        "  target_dataset = np.concatenate((target_dataset,salt_images))\n",
        "  for i in range(len(pepper_images)):\n",
        "    flattened_image = pepper_images[i].flatten()\n",
        "    for j in range(len(flattened_image)):\n",
        "      p = random.uniform(0,1)\n",
        "      if p <= 0.8:\n",
        "        flattened_image[j] = 1\n",
        "    pepper_images[i] = np.reshape(flattened_image,(28,28))\n",
        "  target_dataset = np.concatenate((target_dataset,pepper_images))\n",
        "  return target_dataset\n",
        "\n",
        "# function to prepare the dataset for a given digit\n",
        "def prepare_training_data(dataset_images,dataset_labels,class_num,amount):\n",
        "    indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "    indices = indices.flatten()\n",
        "    # get the images for making positive dataset\n",
        "    dataset_images_positive = dataset_images[indices] # images consisting of positive class\n",
        "    dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "    # print(f\"Positive train data shape: {dataset_images_positive.shape}\")\n",
        "    # get the images for making negative dataset\n",
        "    dataset_images_negative = np.empty((0,28,28),dtype=np.float32)\n",
        "    dataset_images_negative = put_salt(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    # print(f\"Negative train data shape: {dataset_images_negative.shape}\")\n",
        "    dataset_images_negative = put_pepper(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    # print(f\"Negative train data shape: {dataset_images_negative.shape}\")\n",
        "    dataset_images_negative = get_inverted_data(dataset_images,dataset_labels,dataset_images_negative,class_num)\n",
        "    # print(f\"Negative train data shape: {dataset_images_negative.shape}\")\n",
        "    dataset_images_negative = add_negative_data(dataset_images,dataset_labels,dataset_images_negative,class_num,amount)\n",
        "    print(f\"Negative train data shape: {dataset_images_negative.shape}\")\n",
        "    dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "    # concatenate the negative and positive datasets\n",
        "    modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "    modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "    tensor_x = torch.Tensor(modified_dataset_images)\n",
        "    tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "    new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "    return new_dataset\n",
        "\n",
        "def prepare_testing_data(dataset_images,dataset_labels,class_num):\n",
        "  positive_indices = np.asarray(np.where(dataset_labels == class_num)) # indices of occurrence of digit as label\n",
        "  positive_indices = positive_indices.flatten()\n",
        "  # get the images for making positive dataset\n",
        "  dataset_images_positive = dataset_images[positive_indices] # images consisting of positive class\n",
        "  dataset_labels_positive = generate_labels(dataset_images_positive,0) # generate the class labels\n",
        "  # print(f\"Positive test data shape: {dataset_images_positive.shape}\")\n",
        "  # get the images for making negative classes for testing\n",
        "  negative_indices = np.asarray(np.where(((dataset_labels != class_num))))\n",
        "  negative_indices = negative_indices.flatten()\n",
        "  # negative_indices = negative_indices[:1000]\n",
        "  # get the images for making negative dataset\n",
        "  dataset_images_negative = dataset_images[negative_indices]\n",
        "  dataset_labels_negative = generate_labels(dataset_images_negative,1)\n",
        "  # print(f\"Negative test data shape: {dataset_images_negative.shape}\")\n",
        "  # concatenate the negative and positive datasets\n",
        "  modified_dataset_images = np.concatenate((dataset_images_positive,dataset_images_negative))\n",
        "  modified_dataset_labels = np.concatenate((dataset_labels_positive,dataset_labels_negative))\n",
        "  tensor_x = torch.Tensor(modified_dataset_images)\n",
        "  tensor_y = torch.Tensor(modified_dataset_labels)\n",
        "  new_dataset = TensorDataset(tensor_x,tensor_y.to(torch.int64))\n",
        "  return new_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plotting the data"
      ],
      "metadata": {
        "id": "7S7_9D4BQJNf"
      },
      "id": "7S7_9D4BQJNf"
    },
    {
      "cell_type": "code",
      "source": [
        "class_num = 0\n",
        "amount = 500\n",
        "train_class_set = prepare_training_data(train_images,train_labels,class_num,amount) # fetch the training set for a class\n",
        "test_class_set = prepare_testing_data(test_images,test_labels,class_num)\n",
        "train_loader = DataLoader(train_class_set,batch_size=64,shuffle=True)\n",
        "test_loader = DataLoader(test_class_set,batch_size=64,shuffle=True) # take original testing set of all classes\n",
        "dataiter = iter(train_loader)\n",
        "images,labels = next(dataiter)\n",
        "figure = plt.figure()\n",
        "print(labels)\n",
        "num_of_images = 60\n",
        "for index in range(1,num_of_images+1):\n",
        "  # if labels[index] == 0:\n",
        "  plt.subplot(6,10,index)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(images[index].numpy().squeeze(),cmap='gray')\n",
        "    # break"
      ],
      "metadata": {
        "id": "76KzvLzSQCFy"
      },
      "id": "76KzvLzSQCFy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "78ef3a88",
      "metadata": {
        "id": "78ef3a88"
      },
      "source": [
        "## Build the neural network"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model class\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1,6,5)\n",
        "        self.conv2 = nn.Conv2d(6,16,5)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.fc1 = nn.Linear(16*4*4, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "        self.fc3 = nn.Linear(10, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i8ZNAjNZ-Iri"
      },
      "id": "i8ZNAjNZ-Iri",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "tSL2BWDd-MSY"
      },
      "id": "tSL2BWDd-MSY"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5433be70",
      "metadata": {
        "id": "5433be70"
      },
      "outputs": [],
      "source": [
        "def train(model,trainloader):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.5)\n",
        "  # train the model\n",
        "  model = model.to(device)\n",
        "  model.train()\n",
        "  for e in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch, (images,scores) in enumerate(train_loader):\n",
        "      (images,scores) = (images.to(device),scores.to(device))\n",
        "      optimizer.zero_grad()\n",
        "      # compute prediction error\n",
        "      output = model(images)\n",
        "      loss = criterion(output,scores)\n",
        "      # Backpropagation\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      running_loss += loss.item()\n",
        "    # else:\n",
        "    #   print(f\"Training loss: {running_loss/len(train_loader)}\")\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "BpT0936aAfTz"
      },
      "id": "BpT0936aAfTz"
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,testloader,classes,failed_points=None,passed_points=None,scores=None):\n",
        "  correct = np.zeros(classes,dtype=np.float64)\n",
        "  total = np.zeros(classes,dtype=np.float64)\n",
        "  model = model.to(device)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  with torch.no_grad():\n",
        "    for images,labels in testloader:\n",
        "      (images,labels) = (images.to(device),labels.to(device))\n",
        "      output = model(images)\n",
        "      for i in range(len(labels)):\n",
        "        y_true.append(labels[i].cpu())\n",
        "        yes_score = output[i][0].item()\n",
        "        no_score = output[i][1].item()\n",
        "        pred = 0 if yes_score >= no_score else 1\n",
        "        y_pred.append(pred)\n",
        "        if failed_points is not None and pred != labels[i] and labels[i] == 1 and yes_score >= no_score:\n",
        "          failed_points.append(images[i])\n",
        "  precision = precision_score(y_true,y_pred,pos_label=0)\n",
        "  recall = recall_score(y_true,y_pred,pos_label=0)\n",
        "  f1 = f1_score(y_true,y_pred,pos_label=0)\n",
        "  accuracy = accuracy_score(y_true,y_pred)\n",
        "  return {'accuracy':accuracy,'precision':precision,'recall':recall,'f1':f1}"
      ],
      "metadata": {
        "id": "Yf0lJ-gSAeb5"
      },
      "id": "Yf0lJ-gSAeb5",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model"
      ],
      "metadata": {
        "id": "pNKh_D3E-dXM"
      },
      "id": "pNKh_D3E-dXM"
    },
    {
      "cell_type": "code",
      "source": [
        "# model = NeuralNetwork()\n",
        "# model = train(model,train_loader)"
      ],
      "metadata": {
        "id": "PvoZUQbyNiCp"
      },
      "id": "PvoZUQbyNiCp",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions from the model"
      ],
      "metadata": {
        "id": "7BCuVV6d7gvr"
      },
      "id": "7BCuVV6d7gvr"
    },
    {
      "cell_type": "code",
      "source": [
        "# failed_points = []\n",
        "# passed_points = []\n",
        "average_accuracy = 0\n",
        "average_precision = 0\n",
        "average_recall = 0\n",
        "for i in range(5):\n",
        "  model = NeuralNetwork()\n",
        "  model = train(model,train_loader)\n",
        "  metrics = test(model,test_loader,2)\n",
        "  average_accuracy += metrics['accuracy']\n",
        "  average_precision += metrics['precision']\n",
        "  average_recall += metrics['recall']\n",
        "  print(metrics['accuracy'])\n",
        "print(f\"Accuracy: {average_accuracy/5:.2f}\")\n",
        "print(f\"Precision: {average_precision/5:.2f}\")\n",
        "print(f\"Recall: {average_recall/5:.2f}\")\n",
        "  # print(f\"F1 score: {metrics['f1']}\")"
      ],
      "metadata": {
        "id": "jKMZUGyU-k9F"
      },
      "id": "jKMZUGyU-k9F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot failed data points"
      ],
      "metadata": {
        "id": "OdpzxcDi3S3I"
      },
      "id": "OdpzxcDi3S3I"
    },
    {
      "cell_type": "code",
      "source": [
        "# for index in range(0,min(60,len(failed_points))):\n",
        "# # index = 1\n",
        "#   plt.subplot(6,10,index+1)\n",
        "#   plt.axis('off')\n",
        "#   plt.imshow(failed_points[index].cpu().squeeze(),cmap='gray')"
      ],
      "metadata": {
        "id": "N8cspEZg3Uci"
      },
      "id": "N8cspEZg3Uci",
      "execution_count": 24,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "MNIST classification.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}